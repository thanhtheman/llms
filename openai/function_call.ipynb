{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting 11:15 am\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pprint import pprint\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a test.'\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    # model=\"gpt-4-1106-preview\"\n",
    ")\n",
    "pprint(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"3 validation errors for Request\\nbody -> tools -> 0 -> type\\n  unexpected value; permitted: <ToolTypeParam.CODE_INTERPRETER: 'code_interpreter'> (type=value_error.const; given=function; permitted=(<ToolTypeParam.CODE_INTERPRETER: 'code_interpreter'>,))\\nbody -> tools -> 0 -> type\\n  unexpected value; permitted: <ToolTypeParam.RETRIEVAL: 'retrieval'> (type=value_error.const; given=function; permitted=(<ToolTypeParam.RETRIEVAL: 'retrieval'>,))\\nbody -> tools -> 0 -> function\\n  field required (type=value_error.missing)\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thanh quach\\projects\\llms\\openai\\function_call.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thanh%20quach/projects/llms/openai/function_call.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m assistant \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mbeta\u001b[39m.\u001b[39;49massistants\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thanh%20quach/projects/llms/openai/function_call.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mQuestion Decomposer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thanh%20quach/projects/llms/openai/function_call.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     instructions\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYou are sale representative for a company that sells running shoes. You are talking to a customer who is interested in buying your product. When the customer asks you a question, you must decompose the question into 2 parts: 1) the key data, and 2) the intention of the question. For example, if the customer asks \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mHow much do your shoes cost?\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, the key data is price, and the intention is they have a concern about price.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thanh%20quach/projects/llms/openai/function_call.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tools\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mfunction\u001b[39;49m\u001b[39m\"\u001b[39;49m}],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thanh%20quach/projects/llms/openai/function_call.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4-1106-preview\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thanh%20quach/projects/llms/openai/function_call.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thanh quach\\projects\\llms\\virtual\\lib\\site-packages\\openai\\resources\\beta\\assistants\\assistants.py:95\u001b[0m, in \u001b[0;36mAssistants.create\u001b[1;34m(self, model, description, file_ids, instructions, metadata, name, tools, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mCreate an assistant with a model and instructions.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m extra_headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mOpenAI-Beta\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistants=v1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(extra_headers \u001b[39mor\u001b[39;00m {})}\n\u001b[1;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m     96\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/assistants\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     97\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m     98\u001b[0m         {\n\u001b[0;32m     99\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    100\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m\"\u001b[39;49m: description,\n\u001b[0;32m    101\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfile_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m: file_ids,\n\u001b[0;32m    102\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39minstructions\u001b[39;49m\u001b[39m\"\u001b[39;49m: instructions,\n\u001b[0;32m    103\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[0;32m    104\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m: name,\n\u001b[0;32m    105\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    106\u001b[0m         },\n\u001b[0;32m    107\u001b[0m         assistant_create_params\u001b[39m.\u001b[39;49mAssistantCreateParams,\n\u001b[0;32m    108\u001b[0m     ),\n\u001b[0;32m    109\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    110\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    111\u001b[0m     ),\n\u001b[0;32m    112\u001b[0m     cast_to\u001b[39m=\u001b[39;49mAssistant,\n\u001b[0;32m    113\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thanh quach\\projects\\llms\\virtual\\lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\thanh quach\\projects\\llms\\virtual\\lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    855\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    856\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    859\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thanh quach\\projects\\llms\\virtual\\lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"3 validation errors for Request\\nbody -> tools -> 0 -> type\\n  unexpected value; permitted: <ToolTypeParam.CODE_INTERPRETER: 'code_interpreter'> (type=value_error.const; given=function; permitted=(<ToolTypeParam.CODE_INTERPRETER: 'code_interpreter'>,))\\nbody -> tools -> 0 -> type\\n  unexpected value; permitted: <ToolTypeParam.RETRIEVAL: 'retrieval'> (type=value_error.const; given=function; permitted=(<ToolTypeParam.RETRIEVAL: 'retrieval'>,))\\nbody -> tools -> 0 -> function\\n  field required (type=value_error.missing)\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Question Decomposer\",\n",
    "    instructions=\"You are sale representative for a company that sells running shoes. You are talking to a customer who is interested in buying your product. When the customer asks you a question, you must decompose the question into 2 parts: 1) the key data, and 2) the intention of the question. For example, if the customer asks 'How much do your shoes cost?', the key data is price, and the intention is they have a concern about price.\",\n",
    "    tools=[{\"type\": \"function\"}],\n",
    "    model=\"gpt-4-1106-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
